---
title: "Rare Events"
author: "Daniel K Baissa"
date: '2022-09-15'
pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MLmetrics)
library(stargazer)
#library(scatterplot3d)
# install.packages("clusterSEs")
library(clusterSEs)
# install.packages("Zelig")
# library(Zelig)
# library(BNN)
#install.packages("margins")
library(DataCombine)
library(Hmisc)
library(sandwich)
library(scales)
library(tidyverse)
library(sjPlot)
library(mediation)
library(ggpubr)
library(corrr)
# library(Zelig)
library(lmtest)
library(foreign)
library(pcse)
library(car)
library(regclass)
library(neuralnet)
library(Matrix)
library(robust)
#library(arm)
library(mgcv)
library(splines)
#library(plotrix)
library(MASS)
#library(calibrate)
library(plm)
library(rms)


require(rJava)
options(java.parameters = "-Xmx5g")     # or 8g, or larger than this, ...
require(bartMachine)
set_bart_machine_num_cores(30)
```


# No Variation Critique

We have heard many critiques along the lines of the following:

Finally, and maybe most crucially, the authors employ a variation based approach to explain an empirical picture with very little variation. After all, authoritarian regimes in the Middle East are very persistent throughout. (In light of this more general point, the selection of dependent variables might overestimate variation where there is very little substantially.) 

## Rare Events

So since changes in regime can be rare, how well does BART estimate rare events? We can test that!


## Ideal Data


Let's start by creating a baseline so we can compare the models before moving into rare events. We will start by generating some data.

The functional form of the data will be where y is distributed binomial with probability 

$$ \frac{1}{1 + e^{-z}}$$

and where 

$$ z = -1 + -1\beta_1 + 2\beta_2 $$

So let's generate the data:

```{r}

set.seed(02142)
x1 <- rnorm(1000) # Variable 1
x2 <- rnorm(1000) # Variable 2

z <- -1 + -1*x1 + 2*x2 # linear combination with a constant
pr <- 1/(1+exp(-z)) # pass through an inv-logit function

y <- as.factor(rbinom(1000,1,pr)) 

df <- data.frame(y=y,x1=x1,x2=x2)

```

### Logit

Let's test the Logit Model

```{r}
m1.glm <- glm(y~x1+x2,data=df,family="binomial")

summary(m1.glm)
```
No surprises here. the Logit is more or less accurite. Now let's look at the Logit's confusion Matrix.

#### Accuracy

To measure the accuracy of a model we can use a Confusion Matrix. Basically this tells us how many predictions were correct and how many were false.

```{r}
confusion_matrix(m1.glm)
```

## Rare But Easy

$$ \frac{1}{1 + e^{-z}}$$

and where 

$$ z = -2 + -1\beta_1 + 2\beta_2 $$

```{r}

set.seed(02142)
x1 <- rnorm(1000) # Variable 1
x2 <- rnorm(1000) # Variable 2

z <- -3 + -1*x1 + 2*x2 # linear combination with a constant
pr <- 1/(1+exp(-z)) # pass through an inv-logit function

y <- as.factor(rbinom(1000,1,pr)) 

df <- data.frame(y=y,x1=x1,x2=x2)

length(which(df$y == 1))
```


### Logit

Let's test the Logit Model

```{r}
m1.glm <- glm(y~x1+x2,data=df,family="binomial")

summary(m1.glm)
```
No surprises here. the Logit is more or less accurite. Now let's look at the Logit's confusion Matrix.

#### Accuracy

To measure the accuracy of a model we can use a Confusion Matrix. Basically this tells us how many predictions were correct and how many were false.

```{r}
confusion_matrix(m1.glm)
```



```{r}
nn <- neuralnet(y ~ x1 + x2, data=df, 
                # hidden=c(2,2), 
                hidden=c(3), 

                linear.output=FALSE, 
                threshold=0.01)
plot(nn)
```

### Making a confusion matrix

```{r}
#Test the resulting output
temp_test <- subset(df, select = c("x1","x2"))
head(temp_test)
nn.results <- compute(nn, temp_test)
results <- data.frame(actual = as.numeric(df$y)-1, prediction = nn.results$net.result[,2])

```

```{r}
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
table(roundedresultsdf$actual,roundedresultsdf$prediction)
```


```{r echo=T, results='hide'}


bart <- bartMachine(X = df[,2:3], 
                    y = df[,1])

```

```{r}
bart
```

```{r echo=T, results='hide'}


bart2 <- bartMachine(X = df[,2:3], 
                    y = df[,1],
                    prob_rule_class = 1-138/1000)
bart2
```


# Hard Cases


```{r}

set.seed(11)
n  = 800 
p = 5 ##15 useless predictors 
X = data.frame(matrix(runif(n * p), ncol = p))
z = 2 + tan((X[ ,1]^exp(pi)) * sin(X[,2]))*log((X[,3]/sin(X[ ,4]))) * X[,5]^exp(X[,5])

pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = as.factor(rbinom(n,1,pr))    

df2 <- cbind(y, X)
```





Now that we can see how well these models work in ideal environments, let's see what happens.

Here $$z = 2 + tan\bigl(\beta_1 ^ {e^{\pi}} * sin(\beta_2)\bigr)*log\biggl( \frac{\beta_3}{sin(\beta_4)}\biggr) * \beta_5 ^ {e ^ {\beta_5}}  $$

This is a much more complex model that the simple function that we normally approximate in political science, yet it is much more likely to represent how a complex dynamic system works in the real world.

```{r}
set.seed(11)
n  = 800 
p = 20 ##15 useless predictors 
X = data.frame(matrix(runif(n * p), ncol = p))
z = 2 + tan((X[ ,1]^exp(pi)) * sin(X[,2]))*log((X[,3]/sin(X[ ,4]))) * X[,5]^exp(X[,5])

pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = as.factor(rbinom(n,1,pr))    

df2 <- cbind(y, X)
```

How rare are 0s in this function? The zeros make up about `r (length(df2$y[which(df2$y == 0)])/length(df2$y))*100`% of the data



## Logit

```{r}
m2.glm <- glm(y~., data = df2,family="binomial")
confusion_matrix(m2.glm)

```

It gives us statistically significant results... but the model here for only 1 out of the 5 variables we specified as causing y.

```{r}
# df2$y <- as.numeric(df2$y)-1
# m2.re <- relogit(y~.,data=df2)
# confusion_matrix(m2.re)
```

### Accuracy

Let's try the Confusion Matrix for the in sample fit once again.


```{r}
estimatedResponses <- ifelse(m2.glm$fitted.values<0.5, 0, 1)

ConfusionMatrix(estimatedResponses,df2$y)
```

The model did a great job at predicting 1s but a terrible job at predicting 0s. The model may give an impression that it was accurate, since it did a very good job with the 1s, but overall its not useful since it was basically choosing 1 in `r n-1` cases. This is just for the in-sample data. We can expect out-of-sample predictions to be much worse. 


## BART

Let's start with BART-CV without any tuning for rare events and see how well it performs.

```{r results = FALSE}
df2$y <- as.factor(df2$y)

bm2 <- bartMachine(X = df2[,2:6],y = df2[,1])
```

The hyperparameters selected by this BART-CV method are k = `r bm2$k` and the number of trees = `r bm2$num_trees`

### Accuracy

Confusion Matrix for the BART model

```{r}
bm2$confusion_matrix
```

The results here are not  too surprising. This model also predicted 1 for every case. Its a good guess since most of the data are 1s and it was off only `r .121 *100`% of the time. Yet, this is still not as useful of a model.

## Tuned BART

BART lets us change the threshold for classification which can let us tweak the model for rare events.

```{r results = FALSE}

bm3 <- bartMachine(Xy = df2, prob_rule_class = .1)

```

The hyperparameters selected by this BART-CV method are k = `r bm3$k` and the number of trees = `r bm3$num_trees`


```{r}
bm3$confusion_matrix
```


Now the model fits the data much better overall. It missed approximately 1/2 of the 0s as apposed to 100% of them from the logit.


```{r results = FALSE}

bm4 <- bartMachine(Xy = df2, prob_rule_class = .15)
```

The hyperparameters selected by this BART-CV method are k = `r bm4$k` and the number of trees = `r bm4$num_trees`

```{r}
bm4$confusion_matrix

```

Here the model predicted most of the 0s correctly but incorrectly assigned some 1s as 0s


## Neural Network

Now let's compare a Neural Network with Resilient Backpropagation.
```{r}
n <- neuralnet(y ~ .,
               data = df,
               hidden = 5)
```

```{r}
plot(n)
```
### Accuracy 



```{r}
nn_confusion_matrix <- function(n, df){

nn.results <- compute(n, df)
results <- data.frame(actual = as.numeric(df$y), prediction = nn.results$net.result[,2])

roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
m <- as.data.frame.matrix(table(actual,prediction))

m <- as.data.frame(rbind(
      c(m[1,1], m[1,2], 1-m[1,1]/sum(m[1,])),
      c(m[2,1], m[2,2], 1-m[2,2]/sum(m[2,])),
      c(sum(m[,1]), sum(m[,2]), 1-(m[1,1]+ m[2,2])/sum(m))
      ))

m <- m %>% 
  rename(`predicted 0` = V1) %>%
  rename(`predicted 1` = V2) %>%
  rename(`Model Error` = V3)

rownames(m) <- c("actual 0", "actual 1", "Total")
return(m)
}
```

phi = (sqrt(5)+1)/2

```{r}
nn_confusion_matrix(n=n, df = df)
```

