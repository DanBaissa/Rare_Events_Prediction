---
title: "Critiques and Fixes"
author: "Daniel K Baissa"
date: "4/14/2022"
output:
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MLmetrics)
library(stargazer)
#library(scatterplot3d)
# install.packages("clusterSEs")
library(clusterSEs)
# install.packages("Zelig")
# library(Zelig)
# library(BNN)
#install.packages("margins")
library(DataCombine)
library(Hmisc)
library(sandwich)
library(scales)
library(tidyverse)
library(sjPlot)
library(mediation)
library(ggpubr)
library(corrr)
library(Zelig)
library(lmtest)
library(foreign)
library(pcse)
library(car)
library(regclass)

library(Matrix)
library(robust)
#library(arm)
library(mgcv)
library(splines)
#library(plotrix)
library(MASS)
#library(calibrate)
library(plm)
library(rms)


require(rJava)
options(java.parameters = "-Xmx50g")     # or 8g, or larger than this, ...
require(bartMachine)
set_bart_machine_num_cores(30)
```


# No Variation Critique

We have heard many critiques along the lines of the following:

Finally, and maybe most crucially, the authors employ a variation based approach to explain an empirical picture with very little variation. After all, authoritarian regimes in the Middle East are very persistent throughout. (In light of this more general point, the selection of dependent variables might overestimate variation where there is very little substantially.) 

* Substantively, I think we can explain that the lit was off by not looking for subtle changes that can compound to a change. Ie, authoritarianism dies by a thousand cuts everywhere else in the world. Why is MENA different? Should we show this death by 1000 cuts? Can we cite lots of scholars who show this instead?

## Rare Events

So since changes in regime can be rare, how well does BART estimate rare events? We can test that!


## Ideal Data


Let's start by creating a baseline so we can compare the models before moving into rare events. We will start by generating some data.

The functional form of the data will be where y is distributed binomial with probability 

$$ \frac{1}{1 + e^{-z}}$$

and where 

$$ z = -1 + -1\beta_1 + 2\beta_2 $$

So let's generate the data:

```{r}

set.seed(02142)
x1 <- rnorm(1000) # Variable 1
x2 <- rnorm(1000) # Variable 2

z <- -1 + -1*x1 + 2*x2 # linear combination with a constant
pr <- 1/(1+exp(-z)) # pass through an inv-logit function

y <- as.factor(rbinom(1000,1,pr)) 

df <- data.frame(y=y,x1=x1,x2=x2)

```

### Logit

Let's test the Logit Model

```{r}
m1.glm <- glm(y~x1+x2,data=df,family="binomial")

summary(m1.glm)
```
No surprises here. the Logit is more or less accurite. Now let's look at the Logit's confusion Matrix.

#### Accuracy

To measure the accuracy of a model we can use a Confusion Matrix. Basically this tells us how many predictions were correct and how many were false.

```{r}
confusion_matrix(m1.glm)
```

In this Logit, we predicted 0 correctly 549 times and predicted 0 118 times when the true answer was 1. On the other side we predicted 1 correctly 249 times and predicted a 1 when the true answer was 0 84 times.

Remember, this is an ideal case with no iterations and an n of 1000. The model was correct `r 118 + 84` times or `r (1-((118 + 84)/1000)) * 100`% of the time.

```{r}
df$y <- as.numeric(df$y)-1
m1.re <-relogit(y~x1+x2,data=df)
confusion_matrix(m1.re)
```


## Baysian Additive Regression Trees

Here we will use the BART-CV model which builds a model by cross-validating over a number of hyperparameter choices. 

```{r results = FALSE}
df$y <- as.factor(df$y)
bm <- bartMachineCV(Xy = df)
```

The hyperparameters selected by this BART-CV method are k = `r bm$k` and the number of trees = `r bm$num_trees`


#### Accuracy

Confusion Matrix for the BART model

```{r}
bm$confusion_matrix
```

Here we can see a slight improvement from the Logistic Regression. 

## Hard Cases

Now that we can see how well these models work in ideal environments, let's see what happens.

Here $$z = 2 + tan\bigl(\beta_1 ^ {e^{\pi}} * sin(\beta_2)\bigr)*log\biggl( \frac{\beta_3}{sin(\beta_4)}\biggr) * \beta_5 ^ {e ^ {\beta_5}}  $$

This is a much more complex model that the simple function that we normally approximate in political science, yet it is much more likely to represent how a complex dynamic system works in the real world.

```{r}
set.seed(11)
n  = 800 
p = 20 ##15 useless predictors 
X = data.frame(matrix(runif(n * p), ncol = p))
z = 2 + tan((X[ ,1]^exp(pi)) * sin(X[,2]))*log((X[,3]/sin(X[ ,4]))) * X[,5]^exp(X[,5])

pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = as.factor(rbinom(n,1,pr))    

df2 <- cbind(y, X)
```

How rare are 0s in this function? The zeros make up about `r (length(df2$y[which(df2$y == 0)])/length(df2$y))*100`% of the data



### Logit

```{r}
m2.glm <- glm(y~., data = df2,family="binomial")
confusion_matrix(m2.glm)

```

It gives us statistically significant results... but the model here for only 1 out of the 5 variables we specified as causing y.

```{r}
df2$y <- as.numeric(df2$y)-1
m2.re <-relogit(y~.,data=df2)
confusion_matrix(m2.re)
```

#### Accuracy

Let's try the Confusion Matrix for the in sample fit once again.


```{r}
estimatedResponses <- ifelse(m2.glm$fitted.values<0.5, 0, 1)

ConfusionMatrix(estimatedResponses,df2$y)
```

The model did a great job at predicting 1s but a terrible job at predicting 0s. The model may give an impression that it was accurate, since it did a very good job with the 1s, but overall its not useful since it was basically choosing 1 in `r n-1` cases. This is just for the in-sample data. We can expect out-of-sample predictions to be much worse. 


### BART

Let's start with BART-CV without any tuning for rare events and see how well it performs.

```{r results = FALSE}
df2 $y <- as.factor(df2$y)

bm2 <- bartMachineCV(Xy = df2)
```

The hyperparameters selected by this BART-CV method are k = `r bm2$k` and the number of trees = `r bm2$num_trees`

#### Accuracy

Confusion Matrix for the BART model

```{r}
bm2$confusion_matrix
```

The results here are not  too surprising. This model also predicted 1 for every case. Its a good guess since most of the data are 1s and it was off only `r .121 *100`% of the time. Yet, this is still not as useful of a model.

### Tuned BART

BART lets us change the threshold for classification which can let us tweak the model for rare events.

```{r results = FALSE}

bm3 <- bartMachineCV(Xy = df2, prob_rule_class = .1)

```

The hyperparameters selected by this BART-CV method are k = `r bm3$k` and the number of trees = `r bm3$num_trees`


```{r}
bm3$confusion_matrix
```


Now the model fits the data much better overall. It missed approximately 1/2 of the 0s as apposed to 100% of them from the logit.


```{r results = FALSE}

bm4 <- bartMachineCV(Xy = df2, prob_rule_class = .15)
```

The hyperparameters selected by this BART-CV method are k = `r bm4$k` and the number of trees = `r bm4$num_trees`

```{r}
bm4$confusion_matrix

```

Here the model predicted most of the 0s correctly but incorrectly assigned some 1s as 0s

# Causal Infernce

* (R1) First, it does not provide strong enough evidence that the relationship it identifies has a causal interpretation. As the authors acknowledge, the main advantage of tree-based and machine learning approaches is their ability to make out-of-sample predictions; these models cannot by themselves identify causal effects. The authors perform an instrumental variable regression to address this issue, which is nice but not entirely convincing. 

* (R1) Relatedly, I would be curious to know how much of the variation in regime stability small arms and concessional loans explain. If these two variables predict most of the variation in regime stability in the MENA, this would be an important finding that is worth emphasizing. Even if the relationship cannot be demonstrated to be causal, the finding of a strong descriptive pattern is an important contribution. Perhaps the article should lean more heavily on machine learning, and less on hypothesis testing.

# Technical details

* (R1) Second and more importantly, the article does not provide much detail about the BART procedure and how it was implemented. Which prior hyperparameters were used? Did the authors use the default values proposed by Chipman, George, and McCulloch (2010), or a different set of values? Are results sensitive to modifying the hyperparameters? Can the predictive power of the model be assessed (through a root mean squared prediction error, for instance)? In other words, is there a way of validating the machine learning procedure? 

# Clarifying Work

* The revolutions variable apparently contains a whole range of phenomena, including successful revolutions as well as mere attempts, which makes it conceptually ambiguous. (On a 0/1 coding? Again, the variable is not explained very clearly.) 


# DV

* (R2)  There is a long discussion to be had whether a thwarted revolutionary attempt is not a sign of an authoritarian regime's resilience, rather than its instability

* (R2) The general design of the study would benefit from a more varied sample, preferably a global dataset.

# Why BART

* (R2) The BART-method seems sophisticated, but again the authors do not explain or justify it. -- (Dan) Did R2 read the why BART section? R1 and R3 clearly did.... But we can show why BART using a simulation

# Tidying up (R1)

- From Figure A.22, Saudi Arabia is by far the largest recipient of small arms; it also experienced little unrest. Do results hold when removing Saudi Arabia from the sample?

- It is not clear how the authors can assess the influence of Muslim religion, given that there is very little variation among cases.

- Do the results hold when using alternative measures of regime breakdown. For instance, do they hold when using the datasets by Geddes, Wright and Frantz (2014)?

- Figure 7a and 7b appear to be the same.

- Figures 6 and 7 should specify that the y-axis represents predicted probabilities

- Do results hold when using country fixed or random effects, and when using a linear probability model instead of a logit?

#### They may want to see interactions

# Rare but simple


$$ z = -1 + -1\beta_1 + 2\beta_2 $$

So let's generate the data:

```{r}

set.seed(02142)
x1 <- rnorm(1000) # Variable 1
x2 <- rnorm(1000) # Variable 2

z <- -1 + -1*x1 + 2*x2 # linear combination with a constant
pr <- 1/(1+exp(-z)) # pass through an inv-logit function

y <- as.factor(rbinom(1000,1,pr)) 

df <- data.frame(y=y,x1=x1,x2=x2)

```
